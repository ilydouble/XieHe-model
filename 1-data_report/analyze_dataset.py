#!/usr/bin/env python3
"""
YOLOæ•°æ®é›†åˆ†æè„šæœ¬
åˆ†ææ•°æ®é›†çš„æ•°é‡ã€è´¨é‡ã€åˆ†å¸ƒã€æ ‡æ³¨å®Œæ•´æ€§ç­‰ä¿¡æ¯
æ”¯æŒ: å…³é”®ç‚¹æ£€æµ‹(pose)å’Œå®ä¾‹åˆ†å‰²(seg)æ•°æ®é›†

ä½¿ç”¨æ–¹æ³•:
    python analyze_dataset.py --data_dir ../pose_data --task pose --output_dir ./
    python analyze_dataset.py --data_dir ../seg_data --task seg --output_dir ./
"""

import os
import argparse
import yaml
import json
from pathlib import Path
from collections import defaultdict
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm

# è®¾ç½®å­—ä½“
plt.rcParams['axes.unicode_minus'] = False


class DatasetAnalyzer:
    """YOLOæ•°æ®é›†åˆ†æå™¨"""

    def __init__(self, data_dir: str, task: str = 'pose', output_dir: str = './'):
        """
        Args:
            data_dir: æ•°æ®é›†æ ¹ç›®å½• (åŒ…å«imageså’Œlabelsæ–‡ä»¶å¤¹)
            task: ä»»åŠ¡ç±»å‹ 'pose' æˆ– 'seg'
            output_dir: è¾“å‡ºç›®å½•
        """
        self.data_dir = Path(data_dir)
        self.task = task
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # æ•°æ®é›†é…ç½®
        self.yaml_path = self.data_dir / 'dataset.yaml'
        self.config = self._load_config()

        # ç»Ÿè®¡æ•°æ®
        self.stats = {
            'summary': {},
            'images': [],
            'labels': [],
            'issues': []
        }

    def _load_config(self) -> dict:
        """åŠ è½½dataset.yamlé…ç½®"""
        if self.yaml_path.exists():
            with open(self.yaml_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        return {}

    def analyze(self):
        """æ‰§è¡Œå®Œæ•´åˆ†æ"""
        print("=" * 60)
        print(f"YOLOæ•°æ®é›†åˆ†æ - {self.task.upper()}")
        print(f"æ•°æ®ç›®å½•: {self.data_dir}")
        print("=" * 60)

        # 1. åˆ†æå„ä¸ªsplit
        for split in ['train', 'val', 'test']:
            self._analyze_split(split)

        # 2. ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        self._generate_summary()

        # 3. ç”Ÿæˆå¯è§†åŒ–
        self._generate_visualizations()

        # 4. ä¿å­˜æŠ¥å‘Š
        self._save_report()

        print("\nâœ… åˆ†æå®Œæˆ!")
        print(f"æŠ¥å‘Šä¿å­˜è‡³: {self.output_dir}")

    def _analyze_split(self, split: str):
        """åˆ†æå•ä¸ªæ•°æ®é›†split"""
        images_dir = self.data_dir / 'images' / split
        labels_dir = self.data_dir / 'labels' / split

        if not images_dir.exists():
            print(f"âš ï¸  {split} ç›®å½•ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            return

        print(f"\nğŸ“‚ åˆ†æ {split} é›†...")

        # è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶
        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}
        image_files = [f for f in images_dir.iterdir()
                       if f.suffix.lower() in image_extensions]

        split_stats = {
            'split': split,
            'num_images': len(image_files),
            'num_labels': 0,
            'missing_labels': [],
            'empty_labels': [],
            'image_sizes': [],
            'objects_per_image': [],
            'class_distribution': defaultdict(int),
            'bbox_sizes': [],
            'keypoints_stats': [] if self.task == 'pose' else None,
            'polygon_stats': [] if self.task == 'seg' else None,
        }

        for img_path in tqdm(image_files, desc=f"  å¤„ç† {split}"):
            self._analyze_image(img_path, labels_dir, split_stats)

        self.stats['images'].append(split_stats)

        # æ‰“å°splitç»Ÿè®¡
        print(f"  å›¾åƒæ•°é‡: {split_stats['num_images']}")
        print(f"  æ ‡ç­¾æ•°é‡: {split_stats['num_labels']}")
        print(f"  ç¼ºå¤±æ ‡ç­¾: {len(split_stats['missing_labels'])}")
        print(f"  ç©ºæ ‡ç­¾: {len(split_stats['empty_labels'])}")

    def _analyze_image(self, img_path: Path, labels_dir: Path, split_stats: dict):
        """åˆ†æå•å¼ å›¾åƒåŠå…¶æ ‡ç­¾"""
        # è¯»å–å›¾åƒå°ºå¯¸
        try:
            with Image.open(img_path) as img:
                width, height = img.size
                split_stats['image_sizes'].append({
                    'file': img_path.name,
                    'width': width,
                    'height': height,
                    'aspect_ratio': width / height if height > 0 else 0
                })
        except Exception as e:
            self.stats['issues'].append({
                'type': 'image_read_error',
                'file': str(img_path),
                'error': str(e)
            })
            return

        # æŸ¥æ‰¾å¯¹åº”æ ‡ç­¾æ–‡ä»¶
        label_path = labels_dir / (img_path.stem + '.txt')

        if not label_path.exists():
            split_stats['missing_labels'].append(img_path.name)
            split_stats['objects_per_image'].append(0)
            return

        split_stats['num_labels'] += 1

        # è¯»å–æ ‡ç­¾
        with open(label_path, 'r') as f:
            lines = [l.strip() for l in f.readlines() if l.strip()]

        if not lines:
            split_stats['empty_labels'].append(img_path.name)
            split_stats['objects_per_image'].append(0)
            return

        split_stats['objects_per_image'].append(len(lines))

        # è§£ææ¯ä¸ªç›®æ ‡
        for line in lines:
            self._parse_label_line(line, width, height, split_stats)

    def _parse_label_line(self, line: str, img_w: int, img_h: int, split_stats: dict):
        """è§£æå•è¡Œæ ‡ç­¾"""
        parts = line.split()
        if len(parts) < 5:
            return

        class_id = int(parts[0])
        split_stats['class_distribution'][class_id] += 1

        if self.task == 'pose':
            # YOLO poseæ ¼å¼: class x_center y_center width height kp1_x kp1_y kp1_v ...
            if len(parts) >= 5:
                x_c, y_c, w, h = map(float, parts[1:5])
                split_stats['bbox_sizes'].append({
                    'width': w * img_w,
                    'height': h * img_h,
                    'area': w * h * img_w * img_h,
                    'aspect_ratio': w / h if h > 0 else 0
                })

                # è§£æå…³é”®ç‚¹
                kpts = parts[5:]
                if len(kpts) >= 3:
                    num_kpts = len(kpts) // 3
                    kpt_data = {'num_keypoints': num_kpts, 'visible': 0, 'invisible': 0, 'missing': 0}
                    for i in range(num_kpts):
                        try:
                            v = int(float(kpts[i * 3 + 2]))
                            if v == 2:
                                kpt_data['visible'] += 1
                            elif v == 1:
                                kpt_data['invisible'] += 1
                            else:
                                kpt_data['missing'] += 1
                        except (IndexError, ValueError):
                            kpt_data['missing'] += 1
                    split_stats['keypoints_stats'].append(kpt_data)

        elif self.task == 'seg':
            # YOLO segæ ¼å¼: class x1 y1 x2 y2 ... (å¤šè¾¹å½¢åæ ‡)
            # å‰5ä¸ªå€¼å¯èƒ½åŒ…å«bboxä¿¡æ¯ï¼Œä¹‹åæ˜¯å¤šè¾¹å½¢ç‚¹
            coords = list(map(float, parts[1:]))
            num_points = len(coords) // 2

            if num_points > 0:
                # è®¡ç®—å¤šè¾¹å½¢é¢ç§¯ï¼ˆä½¿ç”¨Shoelaceå…¬å¼ï¼‰
                xs = [coords[i * 2] * img_w for i in range(num_points)]
                ys = [coords[i * 2 + 1] * img_h for i in range(num_points)]

                # Shoelaceå…¬å¼è®¡ç®—é¢ç§¯
                area = 0.5 * abs(sum(xs[i] * ys[(i + 1) % num_points] -
                                     xs[(i + 1) % num_points] * ys[i]
                                     for i in range(num_points)))

                split_stats['polygon_stats'].append({
                    'num_points': num_points,
                    'area': area,
                    'bbox_w': max(xs) - min(xs) if xs else 0,
                    'bbox_h': max(ys) - min(ys) if ys else 0
                })

                # ä¼°ç®—bbox
                if xs and ys:
                    w = max(xs) - min(xs)
                    h = max(ys) - min(ys)
                    split_stats['bbox_sizes'].append({
                        'width': w,
                        'height': h,
                        'area': area,
                        'aspect_ratio': w / h if h > 0 else 0
                    })

    def _generate_summary(self):
        """ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡"""
        total_images = sum(s['num_images'] for s in self.stats['images'])
        total_labels = sum(s['num_labels'] for s in self.stats['images'])
        total_missing = sum(len(s['missing_labels']) for s in self.stats['images'])
        total_empty = sum(len(s['empty_labels']) for s in self.stats['images'])

        # åˆå¹¶æ‰€æœ‰ç±»åˆ«åˆ†å¸ƒ
        all_classes = defaultdict(int)
        all_bbox_sizes = []
        all_objects = []
        all_img_sizes = []

        for s in self.stats['images']:
            for cls_id, count in s['class_distribution'].items():
                all_classes[cls_id] += count
            all_bbox_sizes.extend(s['bbox_sizes'])
            all_objects.extend(s['objects_per_image'])
            all_img_sizes.extend(s['image_sizes'])

        # å›¾åƒå°ºå¯¸ç»Ÿè®¡
        if all_img_sizes:
            widths = [s['width'] for s in all_img_sizes]
            heights = [s['height'] for s in all_img_sizes]
            img_size_stats = {
                'width_min': min(widths),
                'width_max': max(widths),
                'width_mean': np.mean(widths),
                'height_min': min(heights),
                'height_max': max(heights),
                'height_mean': np.mean(heights),
            }
        else:
            img_size_stats = {}

        # ç›®æ ‡å°ºå¯¸ç»Ÿè®¡
        if all_bbox_sizes:
            areas = [b['area'] for b in all_bbox_sizes]
            bbox_stats = {
                'area_min': min(areas),
                'area_max': max(areas),
                'area_mean': np.mean(areas),
                'area_median': np.median(areas),
            }
        else:
            bbox_stats = {}

        self.stats['summary'] = {
            'task': self.task,
            'data_dir': str(self.data_dir),
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'total_images': total_images,
            'total_labels': total_labels,
            'total_objects': sum(all_objects),
            'missing_labels': total_missing,
            'empty_labels': total_empty,
            'label_completeness': f"{(total_labels / total_images * 100):.2f}%" if total_images > 0 else "N/A",
            'avg_objects_per_image': np.mean(all_objects) if all_objects else 0,
            'num_classes': len(all_classes),
            'class_distribution': dict(all_classes),
            'image_size_stats': img_size_stats,
            'bbox_size_stats': bbox_stats,
            'splits': {s['split']: s['num_images'] for s in self.stats['images']},
            'issues': self.stats['issues']
        }

        # å…³é”®ç‚¹ç»Ÿè®¡
        if self.task == 'pose':
            all_kpt_stats = []
            for s in self.stats['images']:
                if s['keypoints_stats']:
                    all_kpt_stats.extend(s['keypoints_stats'])

            if all_kpt_stats:
                total_visible = sum(k['visible'] for k in all_kpt_stats)
                total_invisible = sum(k['invisible'] for k in all_kpt_stats)
                total_missing = sum(k['missing'] for k in all_kpt_stats)
                total_kpts = total_visible + total_invisible + total_missing

                self.stats['summary']['keypoint_stats'] = {
                    'total_keypoints': total_kpts,
                    'visible_rate': f"{(total_visible / total_kpts * 100):.2f}%" if total_kpts > 0 else "N/A",
                    'invisible_rate': f"{(total_invisible / total_kpts * 100):.2f}%" if total_kpts > 0 else "N/A",
                    'missing_rate': f"{(total_missing / total_kpts * 100):.2f}%" if total_kpts > 0 else "N/A",
                }

        # åˆ†å‰²ç»Ÿè®¡
        if self.task == 'seg':
            all_poly_stats = []
            for s in self.stats['images']:
                if s['polygon_stats']:
                    all_poly_stats.extend(s['polygon_stats'])

            if all_poly_stats:
                num_points = [p['num_points'] for p in all_poly_stats]
                self.stats['summary']['polygon_stats'] = {
                    'avg_points_per_mask': np.mean(num_points),
                    'min_points': min(num_points),
                    'max_points': max(num_points),
                }


    def _generate_visualizations(self):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        print("\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")

        # æ”¶é›†æ‰€æœ‰æ•°æ®
        all_objects = []
        all_bbox_sizes = []
        all_img_sizes = []
        all_classes = defaultdict(int)
        split_counts = {}

        for s in self.stats['images']:
            split_counts[s['split']] = s['num_images']
            all_objects.extend(s['objects_per_image'])
            all_bbox_sizes.extend(s['bbox_sizes'])
            all_img_sizes.extend(s['image_sizes'])
            for cls_id, count in s['class_distribution'].items():
                all_classes[cls_id] += count

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))
        fig.suptitle(f'Dataset Analysis Report - {self.task.upper()}', fontsize=16, fontweight='bold')

        # 1. æ•°æ®é›†åˆ’åˆ†é¥¼å›¾
        ax1 = axes[0, 0]
        if split_counts:
            labels = list(split_counts.keys())
            sizes = list(split_counts.values())
            colors = ['#66b3ff', '#99ff99', '#ffcc99'][:len(labels)]
            ax1.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)
            ax1.set_title('Dataset Split')

        # 2. æ¯å¼ å›¾åƒç›®æ ‡æ•°é‡åˆ†å¸ƒ
        ax2 = axes[0, 1]
        if all_objects:
            ax2.hist(all_objects, bins=max(10, max(all_objects) - min(all_objects) + 1),
                     color='steelblue', edgecolor='black', alpha=0.7)
            ax2.axvline(np.mean(all_objects), color='red', linestyle='--',
                       label=f'Mean: {np.mean(all_objects):.2f}')
            ax2.set_xlabel('Objects per Image')
            ax2.set_ylabel('Number of Images')
            ax2.set_title('Objects Distribution')
            ax2.legend()

        # 3. ç±»åˆ«åˆ†å¸ƒ
        ax3 = axes[0, 2]
        if all_classes:
            class_names = self.config.get('names', [f'class_{i}' for i in all_classes.keys()])
            class_ids = sorted(all_classes.keys())
            counts = [all_classes[i] for i in class_ids]
            names = [class_names[i] if i < len(class_names) else f'class_{i}' for i in class_ids]

            bars = ax3.bar(range(len(class_ids)), counts, color='coral', edgecolor='black')
            ax3.set_xticks(range(len(class_ids)))
            ax3.set_xticklabels(names, rotation=45, ha='right')
            ax3.set_xlabel('Class')
            ax3.set_ylabel('Count')
            ax3.set_title('Class Distribution')

            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, count in zip(bars, counts):
                ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                        str(count), ha='center', va='bottom', fontsize=8)

        # 4. å›¾åƒå°ºå¯¸åˆ†å¸ƒ
        ax4 = axes[1, 0]
        if all_img_sizes:
            widths = [s['width'] for s in all_img_sizes]
            heights = [s['height'] for s in all_img_sizes]
            ax4.scatter(widths, heights, alpha=0.6, c='green', edgecolors='darkgreen')
            ax4.set_xlabel('Image Width (pixels)')
            ax4.set_ylabel('Image Height (pixels)')
            ax4.set_title('Image Size Distribution')
            ax4.grid(True, alpha=0.3)

        # 5. ç›®æ ‡å°ºå¯¸åˆ†å¸ƒ (å®½é«˜æ¯”)
        ax5 = axes[1, 1]
        if all_bbox_sizes:
            aspect_ratios = [b['aspect_ratio'] for b in all_bbox_sizes if b['aspect_ratio'] > 0]
            if aspect_ratios:
                ax5.hist(aspect_ratios, bins=30, color='purple', edgecolor='black', alpha=0.7)
                ax5.axvline(np.median(aspect_ratios), color='red', linestyle='--',
                           label=f'Median: {np.median(aspect_ratios):.2f}')
                ax5.set_xlabel('Aspect Ratio (W/H)')
                ax5.set_ylabel('Count')
                ax5.set_title('Bbox Aspect Ratio Distribution')
                ax5.legend()

        # 6. ç›®æ ‡é¢ç§¯åˆ†å¸ƒ
        ax6 = axes[1, 2]
        if all_bbox_sizes:
            areas = [b['area'] for b in all_bbox_sizes]
            # ä½¿ç”¨å¯¹æ•°åˆ»åº¦æ›´å¥½åœ°æ˜¾ç¤ºåˆ†å¸ƒ
            areas_log = [np.log10(a) if a > 0 else 0 for a in areas]
            ax6.hist(areas_log, bins=30, color='orange', edgecolor='black', alpha=0.7)
            ax6.set_xlabel('Object Area (log10 pixels^2)')
            ax6.set_ylabel('Count')
            ax6.set_title('Object Area Distribution')

        plt.tight_layout()

        # ä¿å­˜å›¾è¡¨
        fig_path = self.output_dir / f'{self.task}_dataset_analysis.png'
        plt.savefig(fig_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"  ä¿å­˜å›¾è¡¨: {fig_path}")

        # å¦‚æœæ˜¯poseä»»åŠ¡ï¼Œç”Ÿæˆå…³é”®ç‚¹å¯è§æ€§å›¾è¡¨
        if self.task == 'pose':
            self._plot_keypoint_visibility()

    def _plot_keypoint_visibility(self):
        """ç”Ÿæˆå…³é”®ç‚¹å¯è§æ€§å›¾è¡¨"""
        all_kpt_stats = []
        for s in self.stats['images']:
            if s['keypoints_stats']:
                all_kpt_stats.extend(s['keypoints_stats'])

        if not all_kpt_stats:
            return

        # è®¡ç®—æ¯ä¸ªå…³é”®ç‚¹çš„å¯è§æ€§
        num_kpts = all_kpt_stats[0]['num_keypoints'] if all_kpt_stats else 0
        kpt_names = self.config.get('kpt_names', [f'kpt_{i}' for i in range(num_kpts)])

        total_visible = sum(k['visible'] for k in all_kpt_stats)
        total_invisible = sum(k['invisible'] for k in all_kpt_stats)
        total_missing = sum(k['missing'] for k in all_kpt_stats)

        fig, ax = plt.subplots(figsize=(8, 6))
        labels = ['Visible (v=2)', 'Occluded (v=1)', 'Missing (v=0)']
        sizes = [total_visible, total_invisible, total_missing]
        colors = ['#2ecc71', '#f39c12', '#e74c3c']

        ax.pie(sizes, labels=labels, autopct='%1.1f%%', colors=colors, startangle=90)
        ax.set_title('Keypoint Visibility Distribution')

        fig_path = self.output_dir / f'{self.task}_keypoint_visibility.png'
        plt.savefig(fig_path, dpi=150, bbox_inches='tight')
        plt.close()
        print(f"  ä¿å­˜å›¾è¡¨: {fig_path}")

    def _save_report(self):
        """ä¿å­˜åˆ†ææŠ¥å‘Š"""
        print("\nğŸ’¾ ä¿å­˜æŠ¥å‘Š...")

        # ä¿å­˜JSONæŠ¥å‘Š
        json_path = self.output_dir / f'{self.task}_analysis_report.json'

        # å°†numpyç±»å‹è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
        def convert_types(obj):
            if isinstance(obj, dict):
                return {k: convert_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_types(v) for v in obj]
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            return obj

        report_data = convert_types(self.stats['summary'])

        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        print(f"  ä¿å­˜JSONæŠ¥å‘Š: {json_path}")

        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        txt_path = self.output_dir / f'{self.task}_analysis_report.txt'
        self._write_text_report(txt_path)
        print(f"  ä¿å­˜æ–‡æœ¬æŠ¥å‘Š: {txt_path}")

        # ä¿å­˜é—®é¢˜åˆ—è¡¨
        if self.stats['issues']:
            issues_path = self.output_dir / f'{self.task}_issues.json'
            with open(issues_path, 'w', encoding='utf-8') as f:
                json.dump(self.stats['issues'], f, ensure_ascii=False, indent=2)
            print(f"  ä¿å­˜é—®é¢˜åˆ—è¡¨: {issues_path}")

        # ä¿å­˜ç¼ºå¤±/ç©ºæ ‡ç­¾åˆ—è¡¨
        missing_labels = []
        empty_labels = []
        for s in self.stats['images']:
            missing_labels.extend([(s['split'], f) for f in s['missing_labels']])
            empty_labels.extend([(s['split'], f) for f in s['empty_labels']])

        if missing_labels:
            missing_path = self.output_dir / f'{self.task}_missing_labels.txt'
            with open(missing_path, 'w') as f:
                f.write("# ç¼ºå¤±æ ‡ç­¾çš„å›¾åƒåˆ—è¡¨\n")
                f.write("# æ ¼å¼: split, image_file\n\n")
                for split, img in missing_labels:
                    f.write(f"{split}, {img}\n")
            print(f"  ä¿å­˜ç¼ºå¤±æ ‡ç­¾åˆ—è¡¨: {missing_path}")

        if empty_labels:
            empty_path = self.output_dir / f'{self.task}_empty_labels.txt'
            with open(empty_path, 'w') as f:
                f.write("# ç©ºæ ‡ç­¾çš„å›¾åƒåˆ—è¡¨\n")
                f.write("# æ ¼å¼: split, image_file\n\n")
                for split, img in empty_labels:
                    f.write(f"{split}, {img}\n")
            print(f"  ä¿å­˜ç©ºæ ‡ç­¾åˆ—è¡¨: {empty_path}")

    def _write_text_report(self, path: Path):
        """å†™å…¥æ–‡æœ¬æ ¼å¼æŠ¥å‘Š"""
        summary = self.stats['summary']

        lines = [
            "=" * 70,
            f"YOLO æ•°æ®é›†åˆ†ææŠ¥å‘Š - {summary['task'].upper()}",
            "=" * 70,
            f"åˆ†ææ—¶é—´: {summary['analysis_time']}",
            f"æ•°æ®ç›®å½•: {summary['data_dir']}",
            "",
            "-" * 70,
            "ğŸ“Š åŸºæœ¬ç»Ÿè®¡",
            "-" * 70,
            f"æ€»å›¾åƒæ•°é‡:     {summary['total_images']}",
            f"æ€»æ ‡ç­¾æ•°é‡:     {summary['total_labels']}",
            f"æ€»ç›®æ ‡æ•°é‡:     {summary['total_objects']}",
            f"æ ‡ç­¾å®Œæ•´æ€§:     {summary['label_completeness']}",
            f"ç¼ºå¤±æ ‡ç­¾æ•°:     {summary['missing_labels']}",
            f"ç©ºæ ‡ç­¾æ•°:       {summary['empty_labels']}",
            f"å¹³å‡ç›®æ ‡/å›¾åƒ:  {summary['avg_objects_per_image']:.2f}",
            f"ç±»åˆ«æ•°é‡:       {summary['num_classes']}",
            "",
            "-" * 70,
            "ğŸ“ æ•°æ®é›†åˆ’åˆ†",
            "-" * 70,
        ]

        for split, count in summary['splits'].items():
            ratio = count / summary['total_images'] * 100 if summary['total_images'] > 0 else 0
            lines.append(f"  {split:10s}: {count:6d} ({ratio:.1f}%)")

        lines.extend([
            "",
            "-" * 70,
            "ğŸ“ å›¾åƒå°ºå¯¸ç»Ÿè®¡",
            "-" * 70,
        ])

        if summary['image_size_stats']:
            iss = summary['image_size_stats']
            lines.extend([
                f"  å®½åº¦èŒƒå›´: {iss['width_min']:.0f} - {iss['width_max']:.0f} (å¹³å‡: {iss['width_mean']:.0f})",
                f"  é«˜åº¦èŒƒå›´: {iss['height_min']:.0f} - {iss['height_max']:.0f} (å¹³å‡: {iss['height_mean']:.0f})",
            ])

        lines.extend([
            "",
            "-" * 70,
            "ğŸ¯ ç›®æ ‡å°ºå¯¸ç»Ÿè®¡",
            "-" * 70,
        ])

        if summary['bbox_size_stats']:
            bss = summary['bbox_size_stats']
            lines.extend([
                f"  é¢ç§¯èŒƒå›´: {bss['area_min']:.0f} - {bss['area_max']:.0f} pixelsÂ²",
                f"  é¢ç§¯å¹³å‡: {bss['area_mean']:.0f} pixelsÂ²",
                f"  é¢ç§¯ä¸­ä½æ•°: {bss['area_median']:.0f} pixelsÂ²",
            ])

        lines.extend([
            "",
            "-" * 70,
            "ğŸ·ï¸ ç±»åˆ«åˆ†å¸ƒ",
            "-" * 70,
        ])

        class_names = self.config.get('names', [])
        for cls_id, count in sorted(summary['class_distribution'].items()):
            name = class_names[cls_id] if cls_id < len(class_names) else f'class_{cls_id}'
            lines.append(f"  {cls_id}: {name:20s} - {count:6d}")

        # å…³é”®ç‚¹ç»Ÿè®¡
        if 'keypoint_stats' in summary:
            kps = summary['keypoint_stats']
            lines.extend([
                "",
                "-" * 70,
                "ğŸ¦´ å…³é”®ç‚¹ç»Ÿè®¡",
                "-" * 70,
                f"  æ€»å…³é”®ç‚¹æ•°:   {kps['total_keypoints']}",
                f"  å¯è§ç‡:       {kps['visible_rate']}",
                f"  ä¸å¯è§ç‡:     {kps['invisible_rate']}",
                f"  ç¼ºå¤±ç‡:       {kps['missing_rate']}",
            ])

        # åˆ†å‰²ç»Ÿè®¡
        if 'polygon_stats' in summary:
            ps = summary['polygon_stats']
            lines.extend([
                "",
                "-" * 70,
                "ğŸ­ åˆ†å‰²æ©ç ç»Ÿè®¡",
                "-" * 70,
                f"  å¹³å‡ç‚¹æ•°/æ©ç : {ps['avg_points_per_mask']:.1f}",
                f"  æœ€å°‘ç‚¹æ•°:      {ps['min_points']}",
                f"  æœ€å¤šç‚¹æ•°:      {ps['max_points']}",
            ])

        lines.extend([
            "",
            "=" * 70,
            "æŠ¥å‘Šç”Ÿæˆå®Œæˆ",
            "=" * 70,
        ])

        with open(path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))


def main():
    parser = argparse.ArgumentParser(description='YOLOæ•°æ®é›†åˆ†æå·¥å…·')
    parser.add_argument('--data_dir', type=str, required=True,
                        help='æ•°æ®é›†æ ¹ç›®å½• (åŒ…å«imageså’Œlabelsæ–‡ä»¶å¤¹)')
    parser.add_argument('--task', type=str, choices=['pose', 'seg'], default='pose',
                        help='ä»»åŠ¡ç±»å‹: pose(å…³é”®ç‚¹æ£€æµ‹) æˆ– seg(å®ä¾‹åˆ†å‰²)')
    parser.add_argument('--output_dir', type=str, default='./',
                        help='è¾“å‡ºç›®å½• (é»˜è®¤å½“å‰ç›®å½•)')

    args = parser.parse_args()

    analyzer = DatasetAnalyzer(
        data_dir=args.data_dir,
        task=args.task,
        output_dir=args.output_dir
    )
    analyzer.analyze()


if __name__ == '__main__':
    main()
